{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "\n",
    "data_dir = 'D:/ComputerScience/cs_2024_Fall_Deep_Learning/Lab/data/Yelp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = '..\\data\\glove.6B\\glove.6B.300d.txt'  # 确保该文件存在\n",
    "word2vec_output_file = '..\\data\\glove.6B\\glove.6B.300d.word2vec.txt'\n",
    "\n",
    "if not os.path.exists(word2vec_output_file):\n",
    "    glove2word2vec(glove_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "vocab = tokenizer.get_vocab()\n",
    "embedding_dim = 300  # 根据 GloVe 文件选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_path = '../data/glove.6B/pretrained_embedding.pt'\n",
    "if not os.path.exists(pretrained_embedding_path):\n",
    "    embedding_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "    embedding_matrix = np.random.normal(scale=0.6, size=(len(vocab), embedding_dim))  # 随机初始化\n",
    "    index2word = {v: k for k, v in vocab.items()}\n",
    "    # 填充嵌入矩阵\n",
    "    for idx, word in index2word.items():\n",
    "        # 跳过特殊标记\n",
    "        if word in ['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]']:\n",
    "            embedding_matrix[idx] = np.zeros(embedding_dim)\n",
    "            continue\n",
    "\n",
    "        # 处理子词\n",
    "        if word.startswith('##'):\n",
    "            # 子词处理策略：可以选择随机初始化或使用特定方法\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "        else:\n",
    "            if word in embedding_model:\n",
    "                embedding_matrix[idx] = embedding_model[word]\n",
    "            else:\n",
    "                # 未找到的词使用随机向量\n",
    "                embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    pretrained_embedding = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "    torch.save(pretrained_embedding, pretrained_embedding_path)\n",
    "else:\n",
    "    pretrained_embedding = torch.load(pretrained_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data structure\n",
    "@dataclass\n",
    "class YelpData:\n",
    "    text: str\n",
    "    star: int\n",
    "\n",
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, train=True, max_length=512):\n",
    "        \"\"\"\n",
    "        Dataset constructor\n",
    "        :param data_dir: Directory of the data files\n",
    "        :param train: Whether to load training data\n",
    "        :param tokenizer_name: Name of the tokenizer to use\n",
    "        :param max_length: Maximum length for padding and truncation\n",
    "        \"\"\"\n",
    "        self.data_path = os.path.join(data_dir, 'train.json') if train else os.path.join(data_dir, 'test.json')\n",
    "        self.raw_data = self._read_json(self.data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _read_json(self, file_path):\n",
    "        \"\"\"\n",
    "        Load training/test data from the specified directory\n",
    "        :param data_dir: Directory containing the data files\n",
    "        :param train: Whether to load the training data\n",
    "        :return: List of data instances\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                try:\n",
    "                    rdata = json.loads(line)\n",
    "                    text = rdata.get('text', None)\n",
    "                    star = rdata.get('stars', None)\n",
    "                    \n",
    "                    if text is not None and star is not None:\n",
    "                        data.append(YelpData(text=text, star=star))\n",
    "                    else:\n",
    "                        print(f\"{line_num} data is invalid\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Fails to decode line {line_num}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.raw_data[idx].text\n",
    "        label = self.raw_data[idx].star\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.embedding_dim = 0\n",
    "        self.hidden_size = 512\n",
    "        self.num_layers = 2\n",
    "        self.num_classes = 5\n",
    "        self.max_length = 512\n",
    "        self.vocab_size = 0\n",
    "\n",
    "config = Config()\n",
    "config.embedding_dim = embedding_dim\n",
    "config.vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, config, pretrained=None):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=0)\n",
    "        if pretrained is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=config.embedding_dim,\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(config.hidden_size * 2, config.num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x * attention_mask.unsqueeze(-1)\n",
    "        output, _ = self.rnn(x)\n",
    "        x = output[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YelpDataset(data_dir, tokenizer, train=True)\n",
    "test_dataset = YelpDataset(data_dir, tokenizer, train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = RNNClassifier(config, pretrained=pretrained_embedding)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "accuracy = Accuracy(task='multiclass',num_classes=5)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "timenow = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "save_dir = 'checkpoints/' + timenow\n",
    "\n",
    "# 继续你的代码\n",
    "writer = SummaryWriter(log_dir='test_logs')\n",
    "with tqdm(total=num_epochs) as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            accuracy.reset()  # 重置准确率计算\n",
    "\n",
    "            # 训练阶段\n",
    "            with tqdm(total=len(train_loader), desc=f\"Training Epoch {epoch+1}/{num_epochs}\", leave=False) as pbar:\n",
    "                for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item()\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    train_acc += accuracy(preds, labels).item()\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_train_acc = train_acc / len(train_loader)\n",
    "\n",
    "            # 验证阶段\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            test_acc = 0.0\n",
    "            accuracy.reset()  # 重置准确率计算\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(test_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    test_loss += loss.item()\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    test_acc += accuracy(preds, labels).item()\n",
    "\n",
    "            avg_test_loss = test_loss / len(test_loader)\n",
    "            avg_test_acc = test_acc / len(test_loader)\n",
    "\n",
    "            # 记录到 TensorBoard\n",
    "            writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/Train', avg_train_acc, epoch)\n",
    "            writer.add_scalar('Loss/Test', avg_test_loss, epoch)\n",
    "            writer.add_scalar('Accuracy/Test', avg_test_acc, epoch)\n",
    "\n",
    "            # 更新进度条描述\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc:.4f} | \"\n",
    "                f\"Test Loss: {avg_test_loss:.4f} | Test Acc: {avg_test_acc:.4f}\"\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "            # 可选：保存最佳模型\n",
    "            # 你可以根据验证准确率或损失来保存最佳模型\n",
    "            # 这里以验证准确率为例\n",
    "            if epoch == 0:\n",
    "                best_acc = avg_test_acc\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pt'))\n",
    "            else:\n",
    "                if avg_test_acc > best_acc:\n",
    "                    best_acc = avg_test_acc\n",
    "                    torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pt'))\n",
    "                    print(f\"New best model saved at epoch {epoch+1} with accuracy {best_acc:.4f}\")\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
