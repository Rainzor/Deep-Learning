{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"hfl/chinese-bert-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Union, Any\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import set_seed\n",
    "from transformers.data.processors.utils import InputExample, InputFeatures\n",
    "from transformers.data import DefaultDataCollator\n",
    "from transformers import PreTrainedTokenizer, AutoTokenizer, PreTrainedModel, BertForSequenceClassification\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "\n",
    "    model_dir: str = field(\n",
    "        default='chinese-bert-wwm-ext',\n",
    "        metadata={'help': 'The pretrained model directory'}\n",
    "    )\n",
    "    data_dir: str = field(\n",
    "        default='../download',\n",
    "        metadata={'help': 'The data directory'}\n",
    "    )\n",
    "    max_length: int = field(\n",
    "        default=64,\n",
    "        metadata={'help': 'Maximum sequence length allowed to input'}\n",
    "    )\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        self_as_dict = dataclasses.asdict(self)\n",
    "        attrs_as_str = [f\"{k}={v},\\n\" for k, v in sorted(self_as_dict.items())]\n",
    "        return f\"{self.__class__.__name__}(\\n{''.join(attrs_as_str)})\"\n",
    "        \n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "\n",
    "    output_dir: str = field(\n",
    "        default='output_data/',\n",
    "        metadata={'help': 'The output directory where the model predictions and checkpoints will be written.'}\n",
    "    )\n",
    "    train_batch_size: int = field(\n",
    "        default=16,\n",
    "        metadata={'help': 'batch size for training'}\n",
    "    )\n",
    "    eval_batch_size: int = field(\n",
    "        default=32,\n",
    "        metadata={'help': 'batch size for evaluation'}\n",
    "    )\n",
    "    gradient_accumulation_steps: int = field(\n",
    "        default=1,\n",
    "        metadata={'help': 'Number of updates steps to accumulate before performing a backward/update pass.'}\n",
    "    )\n",
    "    num_train_epochs: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"The total number of training epochs\"}\n",
    "    )\n",
    "    learning_rate: float = field(\n",
    "        default=3e-5,\n",
    "        metadata={'help': '\"The initial learning rate for AdamW.'}\n",
    "    )\n",
    "    weight_decay: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Weight decay for AdamW\"}\n",
    "    )\n",
    "    warmup_ratio: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\"help\": \"Linear warmup over warmup_ratio fraction of total steps.\"}\n",
    "    )\n",
    "    dataloader_num_workers: int = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Number of subprocesses to use for data loading (PyTorch only)\"}\n",
    "    )\n",
    "    \n",
    "    logging_steps: int = field(\n",
    "        default=100,\n",
    "        metadata={'help': 'logging states every X updates steps.'}\n",
    "    )\n",
    "    eval_steps: int = field(\n",
    "        default=250,\n",
    "        metadata={'help': 'Run an evaluation every X steps.'}\n",
    "    )\n",
    "    device: str = field(\n",
    "        default='cpu',\n",
    "        metadata={\"help\": 'The device used for training'}\n",
    "    )\n",
    "\n",
    "    def get_warmup_steps(self, num_training_steps):\n",
    "        return int(num_training_steps * self.warmup_ratio)\n",
    "\n",
    "    def __str__(self):\n",
    "        self_as_dict = dataclasses.asdict(self)\n",
    "        attrs_as_str = [f\"{k}={v},\\n\" for k, v in sorted(self_as_dict.items())]\n",
    "        return f\"{self.__class__.__name__}(\\n{''.join(attrs_as_str)})\"\n",
    "        \n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(dataclasses.asdict(self), indent=2) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QQRProcessor:\n",
    "    TASK = 'KUAKE-QQR'\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.task_dir = os.path.join(data_dir)\n",
    "\n",
    "    def get_train_examples(self):\n",
    "        return self._create_examples(os.path.join(self.task_dir, f'{self.TASK}_train.json'))\n",
    "\n",
    "    def get_dev_examples(self):\n",
    "        return self._create_examples(os.path.join(self.task_dir, f'{self.TASK}_dev.json'))\n",
    "\n",
    "    def get_test_examples(self):\n",
    "        return self._create_examples(os.path.join(self.task_dir, f'{self.TASK}_test.json'))\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\", \"2\"]\n",
    "\n",
    "    def _create_examples(self, data_path):\n",
    "\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            samples = json.load(f)\n",
    "\n",
    "        examples = []\n",
    "        for sample in samples:\n",
    "            guid = sample['id']\n",
    "            text_a = sample['query1']\n",
    "            text_b = sample['query2']\n",
    "            label = sample.get('label', None)\n",
    "            if label == '':\n",
    "                label = None\n",
    "\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        examples: List[InputExample],\n",
    "        label_list: List[Union[str, int]],\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        max_length: int = 128,\n",
    "        processor = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.examples = examples\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "\n",
    "        self.label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "        self.id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index) -> InputFeatures:\n",
    "        \n",
    "        example = self.examples[index]\n",
    "        label = None\n",
    "        if example.label is not None and len(example.label) > 0:\n",
    "            label = self.label2id[example.label]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            text=example.text_a,\n",
    "            text_pair=example.text_b,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        feature = InputFeatures(**inputs, label=label)\n",
    "\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer_and_scheduler(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    num_training_steps: int,\n",
    "):\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, \n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_training_steps=num_training_steps, \n",
    "        num_warmup_steps=args.get_warmup_steps(num_training_steps)\n",
    "    )\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_input(data: Union[torch.Tensor, Any], device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(data, Mapping):\n",
    "        return type(data)({k: _prepare_input(v, device) for k, v in data.items()})\n",
    "    elif isinstance(data, (tuple, list)):\n",
    "        return type(data)(_prepare_input(v, device) for v in data)\n",
    "    elif isinstance(data, torch.Tensor):\n",
    "        kwargs = dict(device=device)\n",
    "        return data.to(**kwargs)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_accuracy(preds, labels):\n",
    "\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def evaluate(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    eval_dataloader\n",
    "):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for item in eval_dataloader:\n",
    "        inputs = _prepare_input(item, device=args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            loss = outputs.loss\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "            preds = torch.argmax(outputs.logits.cpu(), dim=-1).numpy()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "            labels_list.append(inputs['labels'].cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds_list, axis=0)\n",
    "    labels = np.concatenate(labels_list, axis=0)\n",
    "    loss = np.mean(loss_list)\n",
    "    accuracy = simple_accuracy(preds, labels)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    train_dataset,\n",
    "    dev_dataset,\n",
    "    data_collator,\n",
    "):\n",
    "\n",
    "    # initialize dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset=train_dataset, \n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset=dev_dataset,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    log_history = {\n",
    "        \"train_loss\": [],\n",
    "        \"eval_loss\": [],\n",
    "        \"eval_accuracy\": []\n",
    "    }\n",
    "\n",
    "    num_examples = len(train_dataloader.dataset)\n",
    "    total_train_batch_size = args.gradient_accumulation_steps * args.train_batch_size\n",
    "    num_update_steps_per_epoch = len(train_dataloader) // args.gradient_accumulation_steps\n",
    "    num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
    "    \n",
    "    max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n",
    "    num_train_epochs = math.ceil(args.num_train_epochs)\n",
    "    num_train_samples = len(train_dataset) * args.num_train_epochs\n",
    "\n",
    "    optimizer, lr_scheduler = create_optimizer_and_scheduler(\n",
    "        args, model, num_training_steps=max_steps\n",
    "    )\n",
    "\n",
    "    print(\"***** Running training *****\")\n",
    "    print(f\"  Num examples = {num_examples}\")\n",
    "    print(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    print(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n",
    "    print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    print(f\"  Total optimization steps = {max_steps}\")\n",
    "\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    global_steps = 0\n",
    "\n",
    "    best_metric = 0.0\n",
    "    best_steps = -1\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for step, item in enumerate(train_dataloader):\n",
    "            inputs = _prepare_input(item, device=args.device)\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            if args.gradient_accumulation_steps > 0:\n",
    "                loss /= args.gradient_accumulation_steps\n",
    "            \n",
    "            loss.backward()\n",
    "            t_loss += loss.detach()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                model.zero_grad()\n",
    "                global_steps += 1\n",
    "\n",
    "                if global_steps % args.logging_steps == 0:\n",
    "                    print(f'Training: Epoch {epoch + 1}/{num_train_epochs} - Step {(step + 1) // args.gradient_accumulation_steps} - Loss {t_loss}')\n",
    "                    log_history['train_loss'].append(t_loss)\n",
    "                t_loss = 0.0\n",
    "\n",
    "            if (global_steps + 1) % args.eval_steps == 0:\n",
    "                \n",
    "                loss, acc = evaluate(args, model, dev_dataloader)\n",
    "                print(f'Evaluation: Epoch {epoch + 1}/{num_train_epochs} - Step {(global_steps + 1) // args.gradient_accumulation_steps} - Loss {loss} - Accuracy {acc}')\n",
    "\n",
    "                log_history['eval_loss'].append(loss)\n",
    "                log_history['eval_accuracy'].append(acc)\n",
    "\n",
    "                if acc > best_metric:\n",
    "                    best_metric = acc\n",
    "                    best_steps = global_steps\n",
    "                    \n",
    "                    saved_dir = os.path.join(args.output_dir, f'checkpoint-{best_steps}')\n",
    "                    os.makedirs(saved_dir, exist_ok=True)\n",
    "                    model.save_pretrained(saved_dir)\n",
    "                    tokenizer.save_vocabulary(save_directory=saved_dir)\n",
    "\n",
    "    return best_steps, best_metric, log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    args: TrainingArguments,\n",
    "    model: PreTrainedModel,\n",
    "    test_dataset,\n",
    "    data_collator\n",
    "):\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    print(\"***** Running prediction *****\")\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "\n",
    "    for item in test_dataloader:\n",
    "        inputs = _prepare_input(item, device=args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "\n",
    "            preds = torch.argmax(outputs.logits.cpu(), dim=-1).numpy()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "    print(f'Prediction Finished!')\n",
    "    preds = np.concatenate(preds_list, axis=0).tolist()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "def generate_commit(output_dir, task_name, test_dataset, preds: List[int]):\n",
    "\n",
    "    test_examples = test_dataset.examples\n",
    "    pred_test_examples = []\n",
    "    for idx in range(len(test_examples)):\n",
    "        example = test_examples[idx]\n",
    "        label  = test_dataset.id2label[preds[idx]]\n",
    "        pred_example = {'id': example.guid, 'query1': example.text_a, 'query2': example.text_b, 'label': label}\n",
    "        pred_test_examples.append(pred_example)\n",
    "    \n",
    "    with open(os.path.join(output_dir, f'{task_name}_test.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(pred_test_examples, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "data_args = DataTrainingArguments(data_dir='../data/KUAKE-QQR')\n",
    "\n",
    "training_args = TrainingArguments()\n",
    "training_args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(data_args)\n",
    "print(training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "\n",
    "\n",
    "# initialize dataset\n",
    "processor = QQRProcessor(data_args.data_dir)\n",
    "train_dataset = ClassificationDataset(\n",
    "    processor.get_train_examples(),\n",
    "    label_list=processor.get_labels(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=data_args.max_length,\n",
    ")\n",
    "dev_dataset = ClassificationDataset(\n",
    "    processor.get_dev_examples(),\n",
    "    label_list=processor.get_labels(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=data_args.max_length,\n",
    ")\n",
    "test_dataset = ClassificationDataset(\n",
    "    processor.get_test_examples(),\n",
    "    label_list=processor.get_labels(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=data_args.max_length,\n",
    ")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "model_name = f'{os.path.split(data_args.model_dir)[-1]}-{str(int(time.time()))}'\n",
    "training_args.output_dir = os.path.join(training_args.output_dir, model_name)\n",
    "if not os.path.exists(training_args.output_dir):\n",
    "    os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(data_args.model_dir, num_labels=len(processor.get_labels()))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-bert-wwm-ext\", num_labels=len(processor.get_labels()))\n",
    "\n",
    "model.to(training_args.device)\n",
    "\n",
    "best_steps, best_metric,log_history = train(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    dev_dataset=dev_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(f'Training Finished! Best step - {best_steps} - Best accuracy {best_metric}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dir = os.path.join(training_args.output_dir, f'checkpoint-{best_steps}')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(best_model_dir, num_labels=len(processor.get_labels()))\n",
    "model.to(training_args.device)\n",
    "\n",
    "model.save_pretrained(training_args.output_dir)\n",
    "torch.save(training_args, os.path.join(training_args.output_dir, 'training_args.bin'))\n",
    "tokenizer.save_vocabulary(save_directory=training_args.output_dir)\n",
    "\n",
    "preds = predict(training_args, model, test_dataset, data_collator)\n",
    "generate_commit(training_args.output_dir, processor.TASK, test_dataset, preds)\n",
    "\n",
    "log_history_path = os.path.join(training_args.output_dir, 'log_history.json')\n",
    "with open(log_history_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(log_history, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "history = json.load(open(log_history_path, 'r', encoding='utf-8'))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history['train_loss'], label='train_loss')\n",
    "plt.plot(history['eval_loss'], label='eval_loss')\n",
    "plt.plot(history['eval_accuracy'], label='eval_accuracy')\n",
    "plt.legend()\n",
    "plt.title('Attention Training History')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss/Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
