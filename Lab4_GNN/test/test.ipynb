{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\runze\\AppData\\Local\\Temp\\ipykernel_17368\\446603893.py:9: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  root = 'D:\\ComputerScience\\cs_2024_Fall_Deep_Learning\\Lab\\data'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid, PPI\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.data.lightning import LightningDataset\n",
    "from torch.utils.data import random_split\n",
    "from typing import Optional, Any, Dict, Tuple\n",
    "import os\n",
    "\n",
    "root = 'D:\\ComputerScience\\cs_2024_Fall_Deep_Learning\\Lab\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_planetoid(root, names):\n",
    "    dataset = Planetoid(root, names)\n",
    "    num_features = dataset.num_features\n",
    "    num_classes = dataset.num_classes\n",
    "    data = dataset[0]\n",
    "\n",
    "    print(f'Dataset: {dataset}:')\n",
    "    print('======================')\n",
    "    print(f'Number of graphs: {len(dataset)}')\n",
    "    print(f'Number of features: {dataset.num_features}')\n",
    "    print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "    print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
    "    print(f'Number of test nodes: {data.test_mask.sum()}')\n",
    "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "    print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_ppi(root):\n",
    "    path = os.path.join(root, 'PPI')\n",
    "    train_dataset = PPI(path, split='train')\n",
    "    val_dataset = PPI(path, split='val')\n",
    "    test_dataset = PPI(path, split='test')\n",
    "    num_features = train_dataset.num_features\n",
    "    num_classes = train_dataset.num_classes\n",
    "\n",
    "    data = train_dataset[0]\n",
    "    train_graphs = len(train_dataset)\n",
    "    node_num = sum([data.num_nodes for data in train_dataset])\n",
    "    edge_num = sum([data.num_edges for data in train_dataset])\n",
    "    \n",
    "\n",
    "    print(f'Dataset: {train_dataset}:')\n",
    "    print('======================')\n",
    "    print(f'Number of train graphs: {len(train_dataset)}')\n",
    "    print(f'Number of val graphs: {len(val_dataset)}')\n",
    "    print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "    print(f'Number of features: {train_dataset.num_features}')\n",
    "    print(f'Number of classes: {train_dataset.num_classes}')\n",
    "    print(f'Avg number of nodes: {node_num / train_graphs:.2f}')\n",
    "    print(f'Avg number of edges: {edge_num / train_graphs:.2f}')\n",
    "    print(f'Average node degree: {edge_num / node_num:.2f}')\n",
    "    # print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "    # print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
    "    # print(f'Number of test nodes: {data.test_mask.sum()}')\n",
    "    # print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "    print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid, PPI\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional, Any, Dict, Tuple\n",
    "import os\n",
    "\n",
    "class GraphDataset:\n",
    "    def __init__(self, dataset_name: str, root: str = 'data/', task='node-cls'):\n",
    "        \"\"\"\n",
    "        Initialize the GraphDataset with the specified dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): Name of the dataset to load. Supported names:\n",
    "                                'Cora', 'CiteSeer', 'PubMed', 'PPI', etc.\n",
    "            root (str): Root directory where the dataset should be saved/downloaded.\n",
    "        \"\"\"\n",
    "        assert task in ['node-cls', 'link-pred'], f\"Task {task} not supported.\"\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.root = root\n",
    "        self.dataset = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.num_features = None\n",
    "        self.num_classes = None\n",
    "        self.task = task\n",
    "        if task == 'node-cls':\n",
    "            self.transform = T.NormalizeFeatures()\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                                T.NormalizeFeatures(),\n",
    "                                T.RandomLinkSplit(\n",
    "                                    num_val=0.05, \n",
    "                                    num_test=0.1, is_undirected=True, add_negative_train_samples=False)]\n",
    "                                )\n",
    "\n",
    "        self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load the dataset based on the dataset name provided during initialization.\n",
    "        \"\"\"\n",
    "        if self.dataset_name in ['cora', 'citeseer']:\n",
    "            self._load_planetoid()\n",
    "        elif self.dataset_name == 'ppi':\n",
    "            self._load_ppi()\n",
    "        else:\n",
    "            assert False, f\"Dataset {self.dataset_name} not supported.\"\n",
    "\n",
    "    def _load_planetoid(self):\n",
    "        \"\"\"\n",
    "        Load Planetoid datasets: Cora, CiteSeer, PubMed.\n",
    "        These datasets come with predefined train/val/test masks.\n",
    "        \"\"\"\n",
    "        dataset_name_cap = self.dataset_name.capitalize()\n",
    "\n",
    "        print(f\"Loaded {dataset_name_cap} dataset:\")\n",
    "        self.dataset = Planetoid(root=self.root, name=dataset_name_cap, transform=self.transform)\n",
    "        \n",
    "        # Set attributes\n",
    "        sample_data = self.dataset[0]\n",
    "        if self.task == 'link-pred':\n",
    "            self.train_dataset, self.val_dataset, self.test_dataset = sample_data\n",
    "            sample_data = self.train_dataset\n",
    "            # only one graph in each dataset\n",
    "            self.train_dataset = [self.train_dataset]\n",
    "            self.val_dataset = [self.val_dataset]\n",
    "            self.test_dataset = [self.test_dataset]\n",
    "        else:\n",
    "            self.train_dataset = self.dataset\n",
    "            self.val_dataset = self.dataset\n",
    "            self.test_dataset = self.dataset\n",
    "\n",
    "            self.num_features = self.dataset.num_node_features\n",
    "            self.num_classes = self.dataset.num_classes\n",
    "            print(f\" - Number of features: {self.num_features}\")\n",
    "            print(f\" - Number of classes: {self.num_classes}\")\n",
    "\n",
    "        print(f\" - Number of training nodes: {sample_data.train_mask.sum()}\")\n",
    "        print(f\" - Number of validation nodes: {sample_data.val_mask.sum()}\")\n",
    "        print(f\" - Number of test nodes: {sample_data.test_mask.sum()}\")\n",
    "        \n",
    "        print(f\" - Number of nodes: {sample_data.num_nodes}\")\n",
    "        print(f\" - Number of edges: {sample_data.num_edges}\")\n",
    "\n",
    "    def _load_ppi(self):\n",
    "        \"\"\"\n",
    "        Load the PPI dataset, which consists of multiple graphs for training, validation, and testing.\n",
    "        \"\"\"\n",
    "        dir_path = os.path.join(self.root, 'PPI')\n",
    "        print(f\"Loaded PPI dataset:\")\n",
    "        self.train_dataset = PPI(root=dir_path, split='train', transform=self.transform)\n",
    "        self.val_dataset = PPI(root=dir_path, split='val', transform=self.transform)\n",
    "        self.test_dataset = PPI(root=dir_path, split='test', transform=self.transform)\n",
    "\n",
    "        if self.task == 'link-pred':\n",
    "            train_dataset = []\n",
    "            val_dataset = []\n",
    "            test_dataset = []\n",
    "\n",
    "            for ta, va, te in self.train_dataset:\n",
    "                train_dataset.append(ta)\n",
    "                val_dataset.append(va)\n",
    "                test_dataset.append(te)\n",
    "            \n",
    "            for ta, va, te in self.val_dataset:\n",
    "                train_dataset.append(ta)\n",
    "                val_dataset.append(va)\n",
    "                test_dataset.append(te)\n",
    "            \n",
    "            for ta, va, te in self.test_dataset:\n",
    "                train_dataset.append(ta)\n",
    "                val_dataset.append(va)\n",
    "                test_dataset.append(te)\n",
    "            \n",
    "            self.train_dataset = train_dataset\n",
    "            self.val_dataset = val_dataset\n",
    "            self.test_dataset = test_dataset\n",
    "\n",
    "        else:\n",
    "            self.num_features = self.train_dataset.num_features\n",
    "            self.num_classes = self.train_dataset.num_classes\n",
    "            print(f'Number of features: {self.train_dataset.num_features}')\n",
    "            print(f'Number of classes: {self.train_dataset.num_classes}')\n",
    "\n",
    "        data = self.train_dataset[0]\n",
    "\n",
    "        print(f'Number of train graphs: {len(self.train_dataset)}')\n",
    "        print(f'Number of val graphs: {len(self.val_dataset)}')\n",
    "        print(f'Number of test graphs: {len(self.test_dataset)}')\n",
    "        print(f'Number of nodes: {data.num_nodes}')\n",
    "        print(f'Number of edges: {data.num_edges}')\n",
    "        print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "        print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "        print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "        print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "    def get_datasets(self) -> Dict[str, Optional[torch.utils.data.Dataset]]:\n",
    "        \"\"\"\n",
    "        Retrieve the datasets based on the dataset type.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing train, val, test, and pred datasets as applicable.\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "        if self.train_dataset is None:\n",
    "            datasets['train'] = self.dataset\n",
    "            datasets['val'] = self.dataset\n",
    "            datasets['test'] = self.dataset\n",
    "        else:  # self.dataset_name == 'ppi':\n",
    "            datasets['train'] = self.train_dataset\n",
    "            datasets['val'] = self.val_dataset\n",
    "            datasets['test'] = self.test_dataset\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PPI dataset:\n",
      "Number of train graphs: 24\n",
      "Number of val graphs: 24\n",
      "Number of test graphs: 24\n",
      "Number of nodes: 1767\n",
      "Number of edges: 27474\n",
      "Average node degree: 15.55\n",
      "Contains isolated nodes: True\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n",
      "24\n",
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "graph_dataset = GraphDataset('ppi', '../data', task='link-pred')\n",
    "datasets = graph_dataset.get_datasets()\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['val']))\n",
    "print(len(datasets['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[11098, 50], edge_index=[2, 276138], y=[11098, 121], edge_label=[138069], edge_label_index=[2, 138069], batch=[11098], ptr=[5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(datasets['train'], batch_size=4, shuffle=True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures(),\n",
    "                        T.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=True, add_negative_train_samples=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: PPI(20):\n",
      "======================\n",
      "Number of train graphs: 20\n",
      "Number of val graphs: 2\n",
      "Number of test graphs: 2\n",
      "Number of features: 50\n",
      "Number of classes: 121\n",
      "Avg number of nodes: 2245.30\n",
      "Avg number of edges: 61318.40\n",
      "Average node degree: 27.31\n",
      "Contains isolated nodes: True\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\runze\\.conda\\envs\\dl\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\runze\\.conda\\envs\\dl\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset_ppi = load_ppi(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Number of validation nodes: 500\n",
      "Number of test nodes: 1000\n",
      "Training node label rate: 0.05\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n",
      "Cora()\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "dataset = load_planetoid(root, 'Cora')[0]\n",
    "dataset_coras = Planetoid(root, 'Cora', transform=transform)\n",
    "print(dataset_coras)\n",
    "print(type(dataset_coras[0]))\n",
    "train_dataset, val_dataset, test_dataset = dataset_coras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[4224], edge_label_index=[2, 4224])\n",
      "Data(x=[2708, 1433], edge_index=[2, 8448], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n",
      "Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054])\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n",
      "1433\n",
      "tensor([1., 1., 1.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.num_nodes)\n",
    "print(test_dataset.num_features)\n",
    "print(test_dataset.edge_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = [(train_dataset.edge_index[0][i], train_dataset.edge_index[1][i]) for i in range(train_dataset.edge_index.size(1))]\n",
    "test_edge_label_index = [(val_dataset.edge_label_index[0][i], val_dataset.edge_label_index[1][i]) for i in range(val_dataset.edge_label_index.size(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n"
     ]
    }
   ],
   "source": [
    "sum_neg = 0\n",
    "for e in test_edge_label_index:\n",
    "    if e not in edge_index:\n",
    "        sum_neg += 1\n",
    "print(sum_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "torch.Size([2708])\n",
      "tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "graph0 = dataset_cora[0]\n",
    "print(graph0)\n",
    "print(graph0.y.size())\n",
    "print(graph0.y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CiteSeer():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 3703\n",
      "Number of classes: 6\n",
      "Number of nodes: 3327\n",
      "Number of edges: 9104\n",
      "Average node degree: 2.74\n",
      "Number of training nodes: 120\n",
      "Number of validation nodes: 500\n",
      "Number of test nodes: 1000\n",
      "Training node label rate: 0.04\n",
      "Contains isolated nodes: True\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\runze\\.conda\\envs\\dl\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\runze\\.conda\\envs\\dl\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset_citeseer = load_planetoid(root, 'CiteSeer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n",
      "torch.Size([3327])\n",
      "tensor([3, 1, 5, 5, 3, 1, 3, 0, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "graph0 = dataset_citeseer[0]\n",
    "print(graph0)\n",
    "print(graph0.y.size())\n",
    "print(graph0.y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root, 'PPI')\n",
    "dataset_ppi0 = PPI(path, transform=transform, split='train') \n",
    "dataset_ppi1 = PPI(path, transform=transform, split='val')\n",
    "dataset_ppi2 = PPI(path, transform=transform, split='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1767, 50], edge_index=[2, 25858], y=[1767, 121], edge_label=[12929], edge_label_index=[2, 12929])\n",
      "Data(x=[1767, 50], edge_index=[2, 25858], y=[1767, 121], edge_label=[3230], edge_label_index=[2, 3230])\n",
      "Data(x=[1767, 50], edge_index=[2, 29088], y=[1767, 121], edge_label=[3230], edge_label_index=[2, 3230])\n"
     ]
    }
   ],
   "source": [
    "for ta, va, te in dataset_ppi0:\n",
    "    print(ta)\n",
    "    print(va)\n",
    "    print(te)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[1767, 50], edge_index=[2, 32318], y=[1767, 121])\n",
      "torch.Size([1767, 121])\n",
      "tensor([[1., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "graph0 = dataset_ppi_train[0]\n",
    "print(graph0)\n",
    "print(graph0.y.shape)\n",
    "print(graph0.y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1433\n",
      "2708\n",
      "10556\n",
      "tensor(140)\n",
      "tensor(500)\n",
      "tensor(1000)\n",
      "torch.Size([2708])\n",
      "1433\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "train_loader = DataLoader(dataset_cora, batch_size=1, shuffle=False)\n",
    "print(dataset_cora.num_classes)\n",
    "print(dataset_cora.num_features)\n",
    "for data in train_loader:\n",
    "    print(data.num_nodes)\n",
    "    print(data.num_edges)\n",
    "    print(data.train_mask.sum())\n",
    "    print(data.val_mask.sum())\n",
    "    print(data.test_mask.sum())\n",
    "    print(data.y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\runze\\AppData\\Local\\Temp\\ipykernel_14552\\1079508957.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.utils as utils\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the GCN convolution layer.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input features per node.\n",
    "            out_channels (int): Number of output features per node.\n",
    "        \"\"\"\n",
    "        # Initialize the MessagePassing class with 'add' aggregation\n",
    "        super(GCNConv, self).__init__(aggr='add')\n",
    "\n",
    "        # Define a linear transformation (weight matrix)\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN layer.\n",
    "\n",
    "        x_i  = W * \\sum_{j \\in N(i)} (1/sqrt(d_i * d_j)) * x_j\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node feature matrix of shape [N, in_channels].\n",
    "            edge_index (LongTensor): Edge indices in COO format of shape [2, E].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features of shape [N, out_channels].\n",
    "        \"\"\"\n",
    "        # Add self-loops to the adjacency matrix\n",
    "        edge_index, _ = utils.add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Compute normalization coefficients\n",
    "        row, col = edge_index\n",
    "        deg = utils.degree(row, x.size(0), dtype=x.dtype)  # Degree of each node\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  # Handle division by zero\n",
    "\n",
    "        # Apply linear transformation\n",
    "        x = self.linear(x)  # [N, out_channels]\n",
    "\n",
    "        # Initiate message passing\n",
    "        return self.propagate(edge_index, x=x, norm=(deg_inv_sqrt, row, col))\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        \"\"\"\n",
    "        Define the message computation.\n",
    "\n",
    "        Args:\n",
    "            x_j (Tensor): Neighbor node features of shape [E, out_channels].\n",
    "            norm (tuple): Tuple containing normalization factors.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Messages to be aggregated.\n",
    "        \"\"\"\n",
    "        deg_inv_sqrt, row, col = norm\n",
    "\n",
    "        # Source node degree (j)\n",
    "        D_j = deg_inv_sqrt[col]\n",
    "        # Target node degree (i)\n",
    "        D_i = deg_inv_sqrt[row]\n",
    "\n",
    "        # Compute normalization factor for each edge\n",
    "        alpha = D_i * D_j\n",
    "\n",
    "        # Scale the messages\n",
    "        return alpha.view(-1, 1) * x_j  # [E, out_channels]\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        \"\"\"\n",
    "        Update node embeddings after aggregation.\n",
    "\n",
    "        Args:\n",
    "            aggr_out (Tensor): Aggregated messages of shape [N, out_channels].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Updated node features.\n",
    "        \"\"\"\n",
    "        return aggr_out\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize a simple GCN model.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input features.\n",
    "            hidden_channels (int): Number of hidden features.\n",
    "            out_channels (int): Number of output features.\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward pass of the GCN model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Node features of shape [N, in_channels].\n",
    "            edge_index (LongTensor): Edge indices in COO format of shape [2, E].\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output features of shape [N, out_channels].\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.planetoid.Planetoid"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_cora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
