{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 with Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "DATASET_PATH = \"../data/\"\n",
    "\n",
    "def get_data_loader(args):\n",
    "    train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
    "    DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0, 1, 2))\n",
    "    DATA_STD = (train_dataset.data / 255.0).std(axis=(0, 1, 2))\n",
    "    print(\"Data mean\", DATA_MEANS)\n",
    "    print(\"Data std\", DATA_STD)\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "    val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "    train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "    _, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "    test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, batch_size=args.batch_size, shuffle=True, drop_last=False, pin_memory=True, num_workers=8,persistent_workers=True)\n",
    "    val_loader = data.DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=8)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=8,persistent_workers=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(pretrained=True, progress=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, imgs):\n",
    "        return self.classifier(self.model(imgs))\n",
    "\n",
    "\n",
    "class CIFARModule(pl.LightningModule):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.args = args\n",
    "        self.model = ResNet50()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        return self.model(imgs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "        scheduler = None\n",
    "        if self.args.optimizer_name == \"Adamw\":\n",
    "            optimizer = optim.AdamW(self.parameters(), lr=self.args.lr)\n",
    "        elif self.args.optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=self.args.lr, momentum=0.9)\n",
    "        \n",
    "        if self.args.scheduler_name == \"lr_schedule\":\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer=optimizer, num_warmup_steps=self.args.warmup_step,\n",
    "                num_training_steps=self.args.total_steps)\n",
    "\n",
    "        if optimizer and scheduler:\n",
    "            return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "        elif optimizer:\n",
    "            return [optimizer]\n",
    "\n",
    "    # Training Step is called for each batch\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"acc/train\", acc, on_step=True)\n",
    "        self.log(\"loss/tain\", loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    # Validation Step is called for each batch\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"acc/val\", acc, on_step=True)\n",
    "        self.log(\"loss/val\", loss, on_step=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = self.loss(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"acc/test\", acc, on_step=True)\n",
    "        self.log(\"loss/test\", loss, on_step=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data mean [0.49139968 0.48215841 0.44653091]\n",
      "Data std [0.24703223 0.24348513 0.26158784]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "pl.seed_everything(42)\n",
    "dt = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_dir = f\"lightning_logs/{dt}\"\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    batch_size: int = 64\n",
    "    lr: float = 1e-3\n",
    "    optimizer_name: str = \"Adamw\"\n",
    "    scheduler_name: str = \"lr_schedule\"\n",
    "    warmup_step: int = 500\n",
    "    total_steps: int = 10000\n",
    "\n",
    "args = Args()\n",
    "train_loader, val_loader, test_loader = get_data_loader(args)\n",
    "model = CIFARModule(args)\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    callbacks=[lr_monitor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode  | In sizes       | Out sizes\n",
      "--------------------------------------------------------------------------------\n",
      "0 | model | ResNet50         | 26.1 M | train | [1, 3, 32, 32] | [1, 10]  \n",
      "1 | loss  | CrossEntropyLoss | 0      | train | ?              | ?        \n",
      "--------------------------------------------------------------------------------\n",
      "517 K     Trainable params\n",
      "25.6 M    Non-trainable params\n",
      "26.1 M    Total params\n",
      "104.299   Total estimated model params size (MB)\n",
      "157       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a3ca604db54824a700ee7705a11bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\runze\\.conda\\envs\\llm\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\runze\\.conda\\envs\\llm\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085306cb77254698b30c5c86434411be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5e74d1db524a5f83eda853a17dc895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\runze\\.conda\\envs\\llm\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:419: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbf29467b1949b2b748bf6ff9af93cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     test_acc_epoch         0.7652000188827515\n",
      "     test_loss_epoch        0.6833807826042175\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc_epoch': 0.7652000188827515, 'test_loss_epoch': 0.6833807826042175}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
