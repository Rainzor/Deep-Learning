{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from models.VGG import *\n",
    "from utils import *\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '..\\\\data\\\\tiny-imagenet-200'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 200\n"
     ]
    }
   ],
   "source": [
    "raw_data = RawData(data_path)\n",
    "\n",
    "\n",
    "num_classes = len(raw_data.labels_t())\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed training data from ..\\data\\tiny-imagenet-200\\process\\train_data.npz...\n",
      "Number of training examples: 100000\n",
      "Shape of the training data: torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TinyImageNetDataset(type_='train', raw_data=raw_data)\n",
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "print(f\"Shape of the training data: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means: tensor([0.4802, 0.4481, 0.3975])\n",
      "Stds: tensor([0.2296, 0.2263, 0.2255])\n"
     ]
    }
   ],
   "source": [
    "means = torch.zeros(3)\n",
    "stds = torch.zeros(3)\n",
    "for img, _ in train_dataset:\n",
    "    means += torch.mean(img, dim=(1, 2))\n",
    "    stds += torch.std(img, dim=(1, 2))\n",
    "\n",
    "means /= len(train_dataset)\n",
    "stds /= len(train_dataset)\n",
    "print(f\"Means: {means}\")\n",
    "print(f\"Stds: {stds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoaderSplit(raw_data, batch_size, val_ratio=0.2, force_reload=False):\n",
    "    \"\"\"\n",
    "    Prepare DataLoaders for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        raw_data (RawData): Instance of the RawData class, providing data and labels.\n",
    "        batch_size (int): Batch size for DataLoaders.\n",
    "        val_ratio (float): Proportion of training data to use for validation.\n",
    "\n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # # Load raw data\n",
    "    # raw_data = RawData(data_path)\n",
    "    # print(\"Raw data loaded, labels: \", len(raw_data.labels_t()))\n",
    "    # Means: tensor([0.4802, 0.4481, 0.3975])\n",
    "    # Stds: tensor([0.2296, 0.2263, 0.2255])\n",
    "    pretrained_means = [0.4802, 0.4481, 0.3975]\n",
    "    pretrained_stds = [0.2296, 0.2263, 0.2255]\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "                            transforms.ToPILImage(),\n",
    "                            transforms.RandomRotation(5),\n",
    "                            transforms.RandomHorizontalFlip(0.5),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=pretrained_means,\n",
    "                                                    std=pretrained_stds)\n",
    "                        ])\n",
    "    test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=pretrained_means,\n",
    "                                                std=pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "    # Create the test dataset from the validation data in the original dataset\n",
    "    test_dataset = TinyImageNetDataset(type_='val', raw_data=raw_data, transform=test_transforms, force_reload=force_reload)\n",
    "    print(\"Validation dataset created, size: \", len(test_dataset))\n",
    "\n",
    "    # Create the full training dataset from the original training data\n",
    "    full_train_dataset = TinyImageNetDataset(type_='train', raw_data=raw_data, transform=train_transforms, force_reload=force_reload)\n",
    "    print(\"Full training dataset created, size: \", len(full_train_dataset))\n",
    "\n",
    "    # Calculate the sizes of the new training and validation sets\n",
    "    full_train_size = len(full_train_dataset)\n",
    "    val_size = int(full_train_size * val_ratio)\n",
    "    train_size = full_train_size - val_size\n",
    "\n",
    "    # Split the dataset into new training and validation datasets\n",
    "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "    print(\"Training and validation datasets created, sizes: \", len(train_dataset), len(val_dataset))\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        x, y = data\n",
    "        print(y)\n",
    "        if i == 10:\n",
    "            break\n",
    "\n",
    "    # Create DataLoaders for train, validation, and test datasets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"DataLoaders created.\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed validation data from ..\\data\\tiny-imagenet-200\\process\\val_data.npz...\n",
      "Validation dataset created, size:  10000\n",
      "Loading preprocessed training data from ..\\data\\tiny-imagenet-200\\process\\train_data.npz...\n",
      "Full training dataset created, size:  100000\n",
      "Training and validation datasets created, sizes:  80000 20000\n",
      "117\n",
      "34\n",
      "23\n",
      "26\n",
      "95\n",
      "57\n",
      "69\n",
      "155\n",
      "95\n",
      "68\n",
      "77\n",
      "DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = DataLoaderSplit(raw_data, batch_size, val_ratio=0.2, force_reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loader size: 625\n",
      "Image shape: torch.Size([128, 3, 64, 64])\n",
      "Label shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "loader_size = len(train_loader)\n",
    "print(f\"Training loader size: {loader_size}\")\n",
    "iterator = iter(train_loader)\n",
    "labels, images = next(iterator)\n",
    "print(f\"Image shape: {images.shape}\")\n",
    "print(f\"Label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    with tqdm(total=len(iterator), desc='Train', leave=False) as t:\n",
    "        for i, (x,label) in enumerate(iterator):\n",
    "            x = x.to(device)\n",
    "            y = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, h = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            t.set_postfix(loss=epoch_loss / (i + 1), acc=epoch_acc / (i + 1))\n",
    "            t.update(1)\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(iterator), desc='Eval', leave=False) as t:\n",
    "            for i, (x, label) in enumerate(iterator):\n",
    "                x = x.to(device)\n",
    "                y = label.to(device)\n",
    "                y_pred, h = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                acc = calculate_accuracy(y_pred, y)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "                t.set_postfix(loss=epoch_loss / (i + 1), acc=epoch_acc / (i + 1))\n",
    "                t.update(1)\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, train_loader, val_loader, optimizer, criterion, scheduler=None, save_best=True, device='cpu'):\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    model = model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    with tqdm(total=num_epochs) as pbar:\n",
    "        for epoch in range(num_epochs):\n",
    "            # Train the model\n",
    "            model.train()\n",
    "            train_loss, train_acc = train(model, train_loader, optimizer, criterion, scheduler, device)\n",
    "\n",
    "            # Evaluate the model\n",
    "            valid_loss, valid_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "            pbar.set_postfix(train_loss=train_loss, valid_loss=valid_loss)\n",
    "\n",
    "            # Save the best model\n",
    "            if valid_acc > best_val_acc:\n",
    "                best_val_acc = valid_acc\n",
    "                best_parms = model.state_dict()\n",
    "            loss_history['train'].append(train_loss)\n",
    "            loss_history['val'].append(valid_loss)\n",
    "            pbar.update(1)\n",
    "            \n",
    "    if save_best:\n",
    "        timestamp = time.strftime(\"%Y_%m_%d_%H_%M\", time.localtime())\n",
    "        torch.save(best_parms, './out/best_model_{}.pth'.format(timestamp))\n",
    "        print('Best model saved as best_model_{}.pth'.format(timestamp))\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)\n",
    "model = VGG(vgg11_layers, num_classes).to(device)\n",
    "\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, train_loader, val_loader, optimizer, criterion, scheduler, save_best, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 9\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     valid_loss, valid_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[35], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, scheduler, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m t\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mepoch_loss \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), acc\u001b[38;5;241m=\u001b[39mepoch_acc \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, num_epochs, train_loader, val_loader, optimizer, criterion, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
